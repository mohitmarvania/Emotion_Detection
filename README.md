# Live Camera Feedback System

**Overview**

The Live Camera Feedback System is a Deep Learning-based project designed for capturing and analyzing emotions in real-time during lectures. It aims to gather valuable feedback from students by assessing their emotional responses. The system records data such as the total number of frames and the percentage of various emotions including Angry, Fear, Disgust, Happy, Surprise, Neutral, and Sad. The results are then stored in an Excel sheet, associating the information with the faculty's name and the current date and time.

**Features**
1. *Real-time Emotion Capture:*
   - Utilizes deep learning algorithms to analyze live camera feed and capture emotions.
   - Seamless integration for faculty members to initiate the emotion capture process.
2. *Data Storage in Excel Sheet:*
   - Records the emotional data in a structured Excel sheet.
   - Information includes faculty name, date, time, and the predominant emotion.
3. *Emotion Statistics:*
   - Calculates the percentage of each emotion based on the total number of frames.
   - Identifies the dominant emotion and highlights it in the Excel sheet.

**Usage**
1. *Input Faculty Information:*
   - Faculty members need to enter their name before starting the emotion capture process.
2. *Start Emotion Capture:*
   - Faculty needs to create an account and then can simply login to the system and start the live camera analysis with emotion detection in real-time.
3. *Automated Data Recording:*
   - The system automatically calculates emotion percentages and records them in an Excel sheet.
4. *Excel Sheet Output:*
   - View the generated Excel sheet for comprehensive feedback, including faculty name, date, time, and dominant emotion.

**Contribution Guidelines**

This project is created by two computer science students which are myself and my classmate [Ruhi_22](https://github.com/Ruhi-22). If you find any issues or have ideas for improvement, please open an issue or submit a pull request.
